{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTraining file: Train a NN to map from seed-> simulation\\nMapping Seed-> Sim (Fig 2)\\n\\nGeneral workflow\\n1) Load train data- input is from images directory and output is latent images from pre-trained Stable Diffusion VAE\\n2) Define NN model \\n3) Run the training and save the model weights\\n4) Plot the training and validation performance \\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training file: Train a NN to map from seed-> simulation\n",
    "Mapping Seed-> Sim (Fig 2)\n",
    "\n",
    "General workflow\n",
    "1) Load train data- input is from images directory and output is latent images from pre-trained Stable Diffusion VAE\n",
    "2) Define NN model \n",
    "3) Run the training and save the model weights\n",
    "4) Plot the training and validation performance \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading input dataset \n",
    "\"\"\"\n",
    "\n",
    "rfactor=256\n",
    "\n",
    "img_length=rfactor\n",
    "img_width=rfactor\n",
    "\n",
    "# for CentOS 8 cluster \n",
    "datadir_o=\"/hpc/group/youlab/ks723/storage/MATLAB_SIMS/Sim_050924/Sim_output\"\n",
    "\n",
    "path_o=os.path.join(datadir_o)\n",
    "\n",
    "input_data=[]\n",
    "\n",
    "# parameters for image\n",
    "img_shape_o=32   # to make image 32x32 after cropping the image \n",
    "\n",
    "\n",
    "\n",
    "# parameters for croppping input seed\n",
    "\n",
    "top_crop_o=0\n",
    "bottom_crop_o=0\n",
    "left_crop_o=3\n",
    "right_crop_o=2\n",
    "\n",
    "#30k images used for training \n",
    "def create_input_data(top_crop,bottom_crop,left_crop,right_crop):\n",
    "    \n",
    "    count=0\n",
    "    img_filenames_o = sorted(os.listdir(path_o))\n",
    "    for img in img_filenames_o:\n",
    "        img_array_o=cv2.imread(os.path.join(path_o,img),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        new_height = img_array_o.shape[0] - (top_crop + bottom_crop)\n",
    "        new_width = img_array_o.shape[1] - (left_crop + right_crop)\n",
    "        new_array_o = img_array_o[top_crop:top_crop+new_height, left_crop:left_crop+new_width]\n",
    "\n",
    "        (T, new_array_o) = cv2.threshold(new_array_o, 0, 255,cv2.THRESH_BINARY| cv2.THRESH_OTSU)\n",
    "\n",
    "        \n",
    "        input_data.append([new_array_o])\n",
    "        count=count+1\n",
    "        if count>=30000:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "create_input_data(top_crop_o,bottom_crop_o,left_crop_o,right_crop_o)\n",
    "\n",
    "\n",
    "y=input_data\n",
    "y=(np.array(y).reshape(-1,1,img_shape_o,img_shape_o)) \n",
    "\n",
    "# normalizing images here to be bw 0 and 1 \n",
    "y=y/255.0\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "y = torch.Tensor(y)\n",
    "y3=y.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading desired output datset- simulated patterns (latent embeddings of this)\n",
    "\"\"\"\n",
    "\n",
    "filepath='/hpc/group/youlab/ks723/miniconda3/Lingchong/Latents'\n",
    "\n",
    "pickle_in=open(os.path.join(filepath,\"latent_dim_75000_4channels_4x32x32_newintermediate102.pickle\"),\"rb\")\n",
    "yprime=pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "yprime=yprime[:30000,:,:,:]\n",
    "\n",
    "\n",
    "yprime=torch.Tensor(yprime)\n",
    "\n",
    "yprime_scaled=yprime\n",
    "yprime_scaled=yprime_scaled.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining dataset \n",
    "\"\"\"\n",
    "\n",
    "# Define train and test datasets\n",
    "dataset = torch.utils.data.TensorDataset(y3, yprime_scaled)\n",
    "\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=64\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining NN model (Here: dilResNet)\n",
    "\"\"\"\n",
    "\n",
    "# Dilated Basic Block similar to PDEArena\n",
    "class PDEArenaDilatedBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, dilation_rates, activation=nn.ReLU, norm=True):\n",
    "        super(PDEArenaDilatedBlock, self).__init__()\n",
    "\n",
    "        # Create dilated convolution layers with specified dilation rates\n",
    "        self.dilated_layers = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_planes if i == 0 else out_planes, \n",
    "                out_planes, \n",
    "                kernel_size=3, \n",
    "                padding=rate, \n",
    "                dilation=rate, \n",
    "                bias=False\n",
    "            )\n",
    "            for i, rate in enumerate(dilation_rates)\n",
    "        ])\n",
    "        \n",
    "        # Normalization and Activation layers\n",
    "        self.norm_layers = nn.ModuleList([nn.BatchNorm2d(out_planes) if norm else nn.Identity() for _ in dilation_rates])\n",
    "        self.activation = activation(inplace=True)\n",
    "\n",
    "        # Shortcut (1x1 convolution if input and output planes differ)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_planes) if norm else nn.Identity()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer, norm in zip(self.dilated_layers, self.norm_layers):\n",
    "            out = self.activation(norm(layer(out)))\n",
    "        return out + self.shortcut(x)  # Residual connection\n",
    "\n",
    "# Dilated ResNet with Adjustable Layers and Blocks\n",
    "class PDEArenaDilatedResNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=64, num_blocks=15, dilation_rates=[1, 2, 4, 8], activation=nn.ReLU, norm=True):\n",
    "        super(PDEArenaDilatedResNet, self).__init__()\n",
    "        \n",
    "        self.in_conv = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)  # Input layer\n",
    "        \n",
    "        # Stack of dilated blocks\n",
    "        self.layers = nn.Sequential(\n",
    "            *[PDEArenaDilatedBlock(hidden_channels, hidden_channels, dilation_rates, activation=activation, norm=norm) for _ in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(hidden_channels, out_channels, kernel_size=3, padding=1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_conv(x)\n",
    "        x = self.layers(x)\n",
    "        return self.out_conv(x)\n",
    "\n",
    "# Example usage\n",
    "model = PDEArenaDilatedResNet(\n",
    "    in_channels=3,               # Input channels \n",
    "    out_channels=4,              # Output channels \n",
    "    hidden_channels=64,          # Number of hidden channels\n",
    "    num_blocks=15,               # Number of dilated blocks \n",
    "    dilation_rates=[1, 2, 4, 8], # Dilation rates for multi-scale feature capture\n",
    "    activation=nn.ReLU,          # Activation function\n",
    "    norm=True                    # Use BatchNorm after each convolution\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total Parameters in Neural Network: 2223620\n",
      "Epoch 1: Significant improvement observed. Best Validation Loss updated to 0.707422.\n",
      "Epoch [2/500] Train Loss: 0.642474 | Test Loss: 0.596892| lr: 0.0001000\n",
      "Epoch 2: Significant improvement observed. Best Validation Loss updated to 0.596892.\n",
      "Epoch 3: Significant improvement observed. Best Validation Loss updated to 0.528459.\n",
      "Epoch [4/500] Train Loss: 0.502367 | Test Loss: 0.469653| lr: 0.0002000\n",
      "Epoch 4: Significant improvement observed. Best Validation Loss updated to 0.469653.\n",
      "Epoch 5: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [6/500] Train Loss: 0.403236 | Test Loss: 0.403484| lr: 0.0003000\n",
      "Epoch 6: Significant improvement observed. Best Validation Loss updated to 0.403484.\n",
      "Epoch 7: Significant improvement observed. Best Validation Loss updated to 0.352359.\n",
      "Epoch [8/500] Train Loss: 0.333812 | Test Loss: 0.349562| lr: 0.0004000\n",
      "Epoch 8: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 9: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch [10/500] Train Loss: 0.288494 | Test Loss: 0.536890| lr: 0.0005000\n",
      "Epoch 10: Validation loss increased beyond tolerance. Epochs without improvement: 2/30.\n",
      "Epoch 11: Validation loss increased beyond tolerance. Epochs without improvement: 3/30.\n",
      "Epoch [12/500] Train Loss: 0.243422 | Test Loss: 1.620113| lr: 0.0004901\n",
      "Epoch 12: Validation loss increased beyond tolerance. Epochs without improvement: 4/30.\n",
      "Epoch 13: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [14/500] Train Loss: 0.212617 | Test Loss: 0.216448| lr: 0.0004803\n",
      "Epoch 14: Significant improvement observed. Best Validation Loss updated to 0.216448.\n",
      "Epoch 15: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch [16/500] Train Loss: 0.185645 | Test Loss: 0.411687| lr: 0.0004707\n",
      "Epoch 16: Validation loss increased beyond tolerance. Epochs without improvement: 2/30.\n",
      "Epoch 17: Validation loss increased beyond tolerance. Epochs without improvement: 3/30.\n",
      "Epoch [18/500] Train Loss: 0.169502 | Test Loss: 0.198337| lr: 0.0004614\n",
      "Epoch 18: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 19: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch [20/500] Train Loss: 0.153392 | Test Loss: 0.798593| lr: 0.0004522\n",
      "Epoch 20: Validation loss increased beyond tolerance. Epochs without improvement: 2/30.\n",
      "Epoch 21: Validation loss increased beyond tolerance. Epochs without improvement: 3/30.\n",
      "Epoch 22: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 23: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 24: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 25: Significant improvement observed. Best Validation Loss updated to 0.139491.\n",
      "Epoch 26: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 27: Validation loss increased beyond tolerance. Epochs without improvement: 2/30.\n",
      "Epoch 28: Validation loss increased beyond tolerance. Epochs without improvement: 3/30.\n",
      "Epoch 29: Validation loss increased beyond tolerance. Epochs without improvement: 4/30.\n",
      "Epoch 30: Validation loss increased beyond tolerance. Epochs without improvement: 5/30.\n",
      "Epoch 31: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 32: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 33: Validation loss increased beyond tolerance. Epochs without improvement: 2/30.\n",
      "Epoch 34: Validation loss increased beyond tolerance. Epochs without improvement: 3/30.\n",
      "Epoch 35: Validation loss increased beyond tolerance. Epochs without improvement: 4/30.\n",
      "Epoch 36: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 37: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 38: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 39: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [40/500] Train Loss: 0.096640 | Test Loss: 0.109000| lr: 0.0003699\n",
      "Epoch 40: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 41: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 42: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 43: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 44: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 45: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 46: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 47: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 48: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 49: Validation loss increased beyond tolerance. Epochs without improvement: 2/30.\n",
      "Epoch 50: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 51: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 52: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 53: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 54: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 55: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 56: Significant improvement observed. Best Validation Loss updated to 0.084730.\n",
      "Epoch 57: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 58: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 59: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 60: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 61: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 62: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 63: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 64: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 65: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 66: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 67: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 68: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 69: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 70: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 71: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 72: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 73: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 74: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 75: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 76: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 77: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 78: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 79: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [80/500] Train Loss: 0.060156 | Test Loss: 0.089486| lr: 0.0002474\n",
      "Epoch 80: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 81: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 82: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 83: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 84: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 85: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 86: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 87: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 88: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 89: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 90: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 91: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 92: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 93: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 94: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 95: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 96: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 97: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 98: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 99: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 100: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 101: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 102: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 103: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 104: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 105: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 106: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 107: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 108: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 109: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 110: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 111: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 112: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 113: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 114: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 115: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 116: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 117: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 118: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 119: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [120/500] Train Loss: 0.049743 | Test Loss: 0.076751| lr: 0.0001655\n",
      "Epoch 120: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 121: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 122: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 123: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 124: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 125: Validation loss increased beyond tolerance. Epochs without improvement: 1/30.\n",
      "Epoch 126: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 127: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 128: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 129: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 130: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 131: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 132: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 133: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 134: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 135: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 136: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 137: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 138: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 139: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 140: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 141: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 142: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 143: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 144: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 145: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 146: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 147: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 148: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 149: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 150: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 151: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 152: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 153: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 154: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 155: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 156: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 157: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 158: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 159: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [160/500] Train Loss: 0.044922 | Test Loss: 0.097860| lr: 0.0001107\n",
      "Epoch 160: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 161: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 162: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 163: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 164: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 165: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 166: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 167: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 168: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 169: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 170: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 171: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 172: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 173: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 174: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 175: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 176: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 177: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 178: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 179: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 180: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 181: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 182: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 183: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 184: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 185: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 186: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 187: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 188: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 189: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 190: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 191: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 192: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 193: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 194: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 195: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 196: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 197: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 198: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 199: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [200/500] Train Loss: 0.041366 | Test Loss: 0.071224| lr: 0.0000741\n",
      "Epoch 200: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 201: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 202: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 203: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 204: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 205: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 206: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 207: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 208: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 209: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 210: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 211: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 212: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 213: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 214: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 215: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 216: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 217: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 218: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 219: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 220: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 221: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 222: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 223: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 224: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 225: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 226: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 227: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 228: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 229: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 230: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 231: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 232: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 233: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 234: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 235: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 236: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 237: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 238: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 239: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [240/500] Train Loss: 0.039189 | Test Loss: 0.068520| lr: 0.0000496\n",
      "Epoch 240: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 241: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 242: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 243: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 244: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 245: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 246: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 247: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 248: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 249: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 250: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 251: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 252: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 253: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 254: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 255: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 256: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 257: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 258: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 259: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 260: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 261: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 262: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 263: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 264: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 265: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 266: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 267: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 268: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 269: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 270: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 271: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 272: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 273: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 274: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 275: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 276: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 277: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 278: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 279: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [280/500] Train Loss: 0.037744 | Test Loss: 0.070285| lr: 0.0000331\n",
      "Epoch 280: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 281: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 282: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 283: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 284: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 285: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 286: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 287: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 288: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 289: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 290: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 291: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 292: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 293: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 294: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 295: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 296: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 297: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 298: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 299: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 300: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 301: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 302: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 303: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 304: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 305: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 306: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 307: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 308: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 309: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 310: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 311: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 312: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 313: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 314: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 315: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 316: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 317: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 318: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 319: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [320/500] Train Loss: 0.036778 | Test Loss: 0.067930| lr: 0.0000222\n",
      "Epoch 320: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 321: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 322: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 323: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 324: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 325: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 326: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 327: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 328: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 329: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 330: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 331: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 332: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 333: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 334: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 335: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 336: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 337: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 338: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 339: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 340: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 341: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 342: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 343: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 344: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 345: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 346: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 347: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 348: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 349: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 350: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 351: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 352: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 353: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 354: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 355: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 356: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 357: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 358: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 359: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [360/500] Train Loss: 0.036126 | Test Loss: 0.068067| lr: 0.0000148\n",
      "Epoch 360: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 361: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 362: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 363: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 364: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 365: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 366: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 367: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 368: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 369: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 370: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 371: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 372: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 373: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 374: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 375: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 376: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 377: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 378: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 379: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 380: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 381: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 382: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 383: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 384: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 385: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 386: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 387: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 388: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 389: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 390: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 391: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 392: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 393: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 394: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 395: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 396: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 397: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 398: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 399: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [400/500] Train Loss: 0.035605 | Test Loss: 0.067847| lr: 0.0000099\n",
      "Epoch 400: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 401: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 402: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 403: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 404: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 405: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 406: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 407: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 408: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 409: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 410: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 411: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 412: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 413: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 414: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 415: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 416: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 417: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 418: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 419: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 420: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 421: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 422: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 423: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 424: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 425: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 426: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 427: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 428: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 429: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 430: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 431: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 432: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 433: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 434: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 435: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 436: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 437: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 438: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 439: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [440/500] Train Loss: 0.035258 | Test Loss: 0.067905| lr: 0.0000066\n",
      "Epoch 440: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 441: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 442: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 443: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 444: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 445: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 446: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 447: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 448: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 449: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 450: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 451: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 452: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 453: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 454: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 455: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 456: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 457: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 458: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 459: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 460: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 461: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 462: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 463: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 464: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 465: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 466: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 467: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 468: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 469: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 470: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 471: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 472: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 473: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 474: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 475: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 476: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 477: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 478: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 479: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [480/500] Train Loss: 0.035010 | Test Loss: 0.068485| lr: 0.0000050\n",
      "Epoch 480: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 481: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 482: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 483: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 484: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 485: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 486: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 487: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 488: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 489: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 490: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 491: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 492: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 493: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 494: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 495: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 496: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 497: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 498: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch 499: Validation loss increased but within tolerance (0.05). Continuing training.\n",
      "Epoch [500/500] Train Loss: 0.035006 | Test Loss: 0.068093| lr: 0.0000050\n",
      "Epoch 500: Validation loss increased but within tolerance (0.05). Continuing training.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model training \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "currentSecond= datetime.now().second\n",
    "currentMinute = datetime.now().minute\n",
    "currentHour = datetime.now().hour\n",
    "currentDay = datetime.now().day\n",
    "currentMonth = datetime.now().month\n",
    "currentYear = datetime.now().year\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 500       \n",
    "warmup_epochs=10\n",
    "lr = 5e-4               \n",
    "min_lr = 5e-6\n",
    "gamma = 0.99\n",
    "\n",
    "# Training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "print(f\"Total Parameters in Neural Network: {count_parameters(model)}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "NAME =f\"Pixel_32x32x3to32x32x4_dilRESNET_30k_newpatterns_seedtointermediate__Model_v{currentMonth}{currentDay}_Cluster_GPU_tfData-{int(time.time())}\"  # change this later to incorporate exact date \n",
    "\n",
    "\n",
    "\n",
    "# Training parameters and early stopping initialization # to save best epoch\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 30\n",
    "delta = 0.05 \n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_params, batch_latents in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch_params.to(device))\n",
    "        loss = criterion(outputs, batch_latents.squeeze(1).to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Warm-up schedule\n",
    "        if epoch < warmup_epochs:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * (epoch + 1) / warmup_epochs\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    # Scheduler step after warmup\n",
    "    if epoch >= warmup_epochs:\n",
    "        scheduler.step()\n",
    "    param_group['lr'] = max(param_group['lr'], min_lr)\n",
    "\n",
    "    # Validation loop for testing set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_params, batch_latents in test_loader:\n",
    "            outputs = model(batch_params.to(device))\n",
    "            loss = criterion(outputs, batch_latents.squeeze(1).to(device))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Store losses in lists\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_val_loss)\n",
    "\n",
    "    interval = 2 if epoch < 20 else 40\n",
    "    if (epoch + 1)%interval == 0 or epoch+1 == num_epochs:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {running_loss / len(train_loader):.6f} | Test Loss: {val_loss / len(test_loader):.6f}| lr: {param_group['lr']:0.7f}\")\n",
    "    \n",
    "    \n",
    "    # Early stopping logic with tolerance\n",
    "    if avg_val_loss < best_loss - delta:\n",
    "        # Significant improvement\n",
    "        best_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        print(f\"Epoch {epoch + 1}: Significant improvement observed. Best Validation Loss updated to {best_loss:.6f}.\")\n",
    "    elif avg_val_loss <= best_loss + delta:\n",
    "        # Within tolerance\n",
    "        epochs_without_improvement = 0\n",
    "        print(f\"Epoch {epoch + 1}: Validation loss increased but within tolerance ({delta}). Continuing training.\")\n",
    "    else:\n",
    "        # Exceeded tolerance\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"Epoch {epoch + 1}: Validation loss increased beyond tolerance. Epochs without improvement: {epochs_without_improvement}/{patience}.\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement after {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# Save the model at the end\n",
    "torch.save(model.state_dict(), f'/hpc/group/youlab/ks723/miniconda3/saved_models/trained/{NAME}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving losses \n",
    "\"\"\"\n",
    "import json\n",
    "# Save losses and model details in a JSON file at the end of training\n",
    "losses = {\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'best_loss': best_loss\n",
    "    # 'saved_model_epoch': saved_model_epoch,\n",
    "    # 'model_name': NAME\n",
    "}\n",
    "\n",
    "with open(f'/hpc/group/youlab/ks723/miniconda3/saved_models/logs/losses_{NAME}.json', 'w') as f:\n",
    "    json.dump(losses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Losses: [1.3440305507296069, 0.64247383071348, 0.5637372968038676, 0.5023674130157272, 0.4499920859438548, 0.40323598372145286, 0.3671832915872194, 0.3338124457552535, 0.31036058435507863, 0.28849369672378655, 0.25942642693694734, 0.2434216383226675, 0.22496395921820148, 0.21261651298446113, 0.19536896240654714, 0.18564512909023684, 0.17499668486592893, 0.16950165957071206, 0.16001309401474859, 0.15339160205628635, 0.14835336694926446, 0.1433914519352088, 0.14090411606911235, 0.13198031598075305, 0.13079768200338734, 0.12588876641192143, 0.12732490674720556, 0.12373080738418475, 0.11624105929692774, 0.11350722972886257, 0.11801621930505993, 0.11324440811482651, 0.10482597070353292, 0.10729038002084217, 0.10205174435230228, 0.09832305256365599, 0.0958686928613491, 0.09598186091294786, 0.10675134559236997, 0.09663975515989896, 0.09317534777084233, 0.09042761714961292, 0.08883610717351968, 0.08868095337002763, 0.08674530879157415, 0.08562877772473046, 0.08707064074150758, 0.08187768181070897, 0.07818407888449198, 0.08175438698999124, 0.07876938433161279, 0.0783570570897717, 0.07536131669700993, 0.07918746040260058, 0.07732927760382964, 0.07330812702741103, 0.07346084770433145, 0.07143926978888104, 0.07301378075333569, 0.07132764594956031, 0.07928945562886118, 0.07059701402358251, 0.06994811517861782, 0.06735789250600112, 0.06974796870485019, 0.07270148222586273, 0.065124080070599, 0.06663975302324193, 0.06634549615613375, 0.06349268073648638, 0.06932123157226644, 0.06838606166437057, 0.06387178037509816, 0.061866594875734565, 0.07112246612307585, 0.062219777157733226, 0.06116292985835912, 0.06173392981114263, 0.06065886792554675, 0.06015649502346584, 0.06035962039648921, 0.06171444471554734, 0.05962659201421444, 0.0594619315965057, 0.05918233209594166, 0.059366457526223354, 0.0632012496344851, 0.06014670865054097, 0.056585642407643855, 0.06230358662904721, 0.057838022382267845, 0.05498453164340761, 0.0580841261116673, 0.0565465774422432, 0.054892719930735245, 0.05602080847238195, 0.05758447407510891, 0.05479400650867354, 0.054567022293241105, 0.05450943896273301, 0.054979638421634364, 0.05505833794226014, 0.05366791685010302, 0.05256856672501112, 0.052353451478679036, 0.05254694292436561, 0.053755956829936016, 0.05444750428129146, 0.0522771101374338, 0.05517447429104439, 0.05326886407112906, 0.05291507996936545, 0.0517637703249946, 0.05154712109769125, 0.05261627282788403, 0.05250281080497667, 0.05449360454569778, 0.05011289066224584, 0.04928041734154473, 0.0497434873186864, 0.04964688455606524, 0.04962912932815145, 0.0500550716409186, 0.05026437457809799, 0.050614801901988506, 0.04921157518622434, 0.050581905575937, 0.04945050451851569, 0.04814702562846561, 0.04992372124186624, 0.0505005037897571, 0.049527033033529164, 0.049126624178264945, 0.04901696392886729, 0.046487902578019415, 0.04697337118087787, 0.051160535294015264, 0.046645927282659365, 0.04819003196865744, 0.047318595668998376, 0.047818359384816406, 0.04675650735213576, 0.04664545037487954, 0.046324341844820295, 0.04651432043003245, 0.04635279063329595, 0.04606147936735108, 0.04699042638084052, 0.04596545928603665, 0.04732099013906237, 0.046454401316024116, 0.047488269241641484, 0.045064847360219436, 0.04636648510940267, 0.04594572797700127, 0.04465409961498179, 0.044593423084130786, 0.044746552810284765, 0.04498558231051217, 0.04492170960417291, 0.045399837619602965, 0.045302782552007814, 0.04488227553489084, 0.044355675798809925, 0.043866709324915265, 0.04418819781692954, 0.04332372050961895, 0.0446644699492302, 0.0441700430560451, 0.044012967903184666, 0.0442715361361255, 0.04367046393630629, 0.04368926492029739, 0.04346232416364254, 0.04299645330668626, 0.04308176100677789, 0.04323779273407437, 0.04358149401102021, 0.0434210957138318, 0.04278182433432595, 0.04365725114376624, 0.042456757533253654, 0.04270584393133767, 0.04303778977226025, 0.04255462019435037, 0.04206218038160372, 0.043199758747177665, 0.04241034506833384, 0.04151513890950318, 0.042778094029892674, 0.04181694859082665, 0.04327638539975571, 0.04201227404459675, 0.04216147757960722, 0.04159902317793731, 0.04130246174208361, 0.04165689927035881, 0.0420799588087187, 0.042169996850651585, 0.041365582645998746, 0.041246925643976265, 0.041417156324073036, 0.04128034475537556, 0.04082738795269157, 0.04147207973551411, 0.041076647047112336, 0.04140392453431801, 0.040569456876807304, 0.0406449850101324, 0.04136451459114585, 0.04075350164837464, 0.04115748521982211, 0.04061897812296429, 0.04083961144230942, 0.040709723246182314, 0.04009528234777575, 0.04094465940272639, 0.04011212914334654, 0.040571844328848104, 0.04055692836359779, 0.0401115198553456, 0.04037867719528234, 0.04020383084519497, 0.04055887572820435, 0.039875472141032535, 0.039495030170862706, 0.03969209591783054, 0.03952196270439297, 0.040023095305497046, 0.03949213759770608, 0.039741153285896044, 0.039774114107139304, 0.03990522992766299, 0.039833886760741614, 0.039184942020553545, 0.03955432459252988, 0.03967493565055714, 0.03894534649689333, 0.03894885820093878, 0.03918932313849858, 0.03965147951909151, 0.03911748697972411, 0.0390632579376771, 0.038978318529318295, 0.03864780352649531, 0.03900660774825026, 0.03894444434104655, 0.03906395017959495, 0.039416342726533445, 0.03885648366046178, 0.03868064580052667, 0.03880695961592322, 0.03896987603279934, 0.03855822494880283, 0.0385825022419482, 0.03853027927755462, 0.03862708055754126, 0.03839841256386013, 0.03843329020586059, 0.03863690646091626, 0.03841105250915362, 0.03844969478652093, 0.038398177552774054, 0.038494292011051943, 0.03821840353521973, 0.03806615431055921, 0.03820922990192734, 0.03831108447640993, 0.03818277082454537, 0.03819419296184705, 0.038044888173093164, 0.037981757664638106, 0.03867267190491984, 0.03845965397054268, 0.03795527629730826, 0.037643998040336565, 0.03783712865369862, 0.03786458869539731, 0.03770898467026898, 0.03774441226928438, 0.03760777579805862, 0.03784055450834087, 0.03749274800562463, 0.03781898016931039, 0.03774949670791344, 0.037626212039053156, 0.03771089395216856, 0.037477178543241105, 0.03745524014094712, 0.03759531711197295, 0.03741657911354049, 0.037359978067931406, 0.03727267987108061, 0.03725315150208948, 0.0373707911732355, 0.03741229786338964, 0.03736538219296537, 0.03769665786122541, 0.037317880786849424, 0.03718696703242747, 0.03712315703808414, 0.03702279707302132, 0.037241564198480966, 0.03718866069770255, 0.03714328153297235, 0.03712152913036222, 0.03700855489979141, 0.03696196760223092, 0.036966201165134875, 0.03700880268456247, 0.03700738432883369, 0.036870231004475985, 0.0370990963237828, 0.03706963245549473, 0.03674981369685505, 0.036828797857908276, 0.0369259059005439, 0.036897256546675875, 0.03662230636730296, 0.03677811890275558, 0.03669619599966359, 0.03658527617841535, 0.03669222427523249, 0.03685246936765045, 0.03662196709222703, 0.03660760595688323, 0.036634790530136976, 0.03668013006731232, 0.03654171740062429, 0.03661861167382008, 0.03645236832553177, 0.03660126426808925, 0.036605771322032854, 0.03650593762926016, 0.03642785875788797, 0.036258454174156435, 0.036433725719344556, 0.03641686072987968, 0.03662183209017837, 0.03642626375101189, 0.0363104506815073, 0.03627977486682164, 0.036383840559995, 0.03623510092955912, 0.03614508253809118, 0.03632374233191047, 0.0363736319404219, 0.036161661545317886, 0.036147493312921004, 0.036147260906007055, 0.03615998553544707, 0.03629205082830095, 0.03626807911489247, 0.03619922432713034, 0.036174167353677525, 0.03606873764810969, 0.036029511744871526, 0.036051178924845294, 0.03604575993396095, 0.03612575172424599, 0.03607827657171618, 0.03601971257177857, 0.03619416886984737, 0.03594154522917564, 0.036027824903410194, 0.035976484285499814, 0.035920325852047776, 0.03591026480517116, 0.035867422324786256, 0.035854099044726355, 0.03590511187240128, 0.03589102722068816, 0.03606135619724814, 0.03592726373736045, 0.03583644018000901, 0.03578525510621015, 0.03576697806399581, 0.035721642702323565, 0.03582792888037966, 0.03565893578232747, 0.03581218920224368, 0.03587165425456531, 0.03565502756438549, 0.03578011485884822, 0.03567485823803603, 0.03569286017374122, 0.03574288420167297, 0.03567577552456427, 0.035638149924823455, 0.0355816853996278, 0.03568451223956748, 0.035612449588439475, 0.03565164962651041, 0.035655482341116075, 0.035575004877143, 0.035573216234620716, 0.035642546235243856, 0.035597850016861166, 0.03548482938788796, 0.035604903209619045, 0.035435487220496366, 0.0356005693110527, 0.0355204371338207, 0.0355002923352176, 0.035536839895974406, 0.035425735187332774, 0.03551439891494281, 0.03557454026635224, 0.0354321525030509, 0.03546285351199844, 0.03538093196837258, 0.03555204805816519, 0.03534573094116003, 0.035512378377513296, 0.03561252925845119, 0.035416063071356564, 0.0354537887154456, 0.035326019229623376, 0.03538624611263874, 0.03535746943682291, 0.03538034663875521, 0.03532926377209159, 0.035384173721286924, 0.03531775633270424, 0.03544046882522332, 0.03532680513346082, 0.035257754079397255, 0.03535533214420504, 0.035393667870788216, 0.03532772382359369, 0.03530705751083191, 0.0353203032395286, 0.03529417504207783, 0.03529647715672215, 0.03535088707838578, 0.03523377379429001, 0.03530863420022608, 0.03524522775545787, 0.03523745570526021, 0.03525834979922003, 0.03526690071757653, 0.03527142940821806, 0.035152906869754405, 0.03532556574173731, 0.03530846992551715, 0.03521681899178367, 0.03504922648812357, 0.03515565111103216, 0.035236398693862685, 0.035165042717945516, 0.03517741770858731, 0.03518704709848521, 0.03505900654924142, 0.035185664795091935, 0.03516119210075993, 0.035107156526632786, 0.03502013694088888, 0.03514879811290316, 0.0351528952171876, 0.035135873783185584, 0.03508717210089426, 0.03508493562904297, 0.03516390706937742, 0.03522937919644383, 0.03515307712964537, 0.035146252733271266, 0.03514055866236088, 0.035095059052462824, 0.035049642475931, 0.03506555694256914, 0.03509098627732546, 0.035051593027338034, 0.03498387502691757, 0.03501205038614748, 0.035238805240223194, 0.03505829347394654, 0.03510328236596562, 0.03504973413396221, 0.035093580465322426, 0.03501014220771066, 0.03501956706857794, 0.03497658897703293, 0.03493508956090534, 0.03498760855346212, 0.035015702741970946, 0.034968431288667765, 0.03501042382023628, 0.034962252214974704, 0.03505115448475166, 0.035025241948063904, 0.034972216669134616, 0.035021341913401796, 0.035056597910715505, 0.03493750141271483, 0.03497926829938922, 0.03503058473822347, 0.035019112600816936, 0.03497903749911706, 0.034876790038923515, 0.03500608096155228]\n",
      "Test Losses: [0.707422394701775, 0.5968923162906727, 0.5284593168725359, 0.46965287911131026, 0.42232773722486294, 0.4034835398197174, 0.35235941790519876, 0.3495616411909144, 0.6486317870464731, 0.5368895219995621, 0.4276550738735402, 1.6201134926461158, 0.33417438002342875, 0.21644845319555162, 0.9123537686276944, 0.41168705072808776, 1.8504414799365592, 0.19833732760967093, 1.5972793885367982, 0.7985925129119386, 1.5971716712129878, 0.20216375874712111, 0.51976906967924, 0.24282368351804448, 0.1394912895370037, 0.41781903169256573, 0.21551923485512428, 0.3287724606851314, 1.6798555055197248, 0.7604844763557962, 0.14232078416550414, 0.22434992850460905, 0.505442886593494, 1.4156547891015703, 0.8336019443070635, 0.10812222323519119, 0.1333036314933858, 1.312571792050879, 0.1513854667861411, 0.10900006585932792, 0.10048759015316659, 0.10385511720434148, 0.3162353217918822, 0.14035466091429932, 0.09224584397483379, 0.09610449919041167, 0.10027411326448968, 0.23144562574143104, 1.7405544085071443, 0.0932884173507386, 2.3691266632460533, 0.1106863372186397, 0.6524008756622355, 0.09452669385899888, 0.09795533453530454, 0.08472956701162014, 0.09905780297010502, 0.09256386344737196, 0.09980544384489669, 4.532563679871407, 0.08838658890825637, 0.08823877905911588, 0.08356646622749085, 0.12565613490469912, 0.08678205200332276, 0.08829589362474198, 0.10504007735785018, 0.08768206692122399, 0.09681380619394019, 0.08437715613461555, 0.0914229359081451, 0.0849557842345948, 0.08135867182244645, 0.09524540143444184, 0.08624763739235858, 0.08341932407719024, 0.08383456395661577, 0.08347170070764866, 0.08246388111976867, 0.08948562452767758, 0.08795831431733801, 0.08241741327529258, 0.12270300550029632, 0.07912879437208176, 0.08326900797955533, 0.07628221556227258, 0.08736960891079396, 0.08010632925211116, 0.07717363377834888, 0.0906107338502052, 0.07604527505154306, 0.3380452854202149, 0.08083378840634163, 0.08156702667474747, 0.08302734459334231, 0.08767880752999732, 0.07770766286139792, 0.08410516650752818, 0.07745205278092242, 0.08539264775971149, 0.07330515473446947, 0.07650937314363236, 0.07865937307794044, 0.07654121359612079, 0.07605361161713904, 0.07893853048060803, 0.08128482927667334, 0.07377652284946848, 0.07768625448992912, 0.1108985663728511, 0.07603951717944855, 0.07632651322699607, 0.08047019801241287, 0.07294471007078251, 0.07521223054921373, 0.07362062801071938, 0.07425515131747469, 0.07390832679068789, 0.07252194890950589, 0.07675086358126174, 0.09664588913004449, 0.08192521428808253, 0.07528747237743215, 0.07374252014337702, 0.34712917230864787, 0.0819461019115245, 0.0719050091314823, 0.07367172662882095, 0.07299908520059382, 0.07399970118669753, 0.0737125279104456, 0.07682508166800155, 0.07360105898152007, 0.07163088229742456, 0.07223776315755033, 0.07617883653716838, 0.07099820245453652, 0.07954100519418716, 0.07540864418161676, 0.07014406266364645, 0.07317191822097656, 0.07217073979529928, 0.0735095165511395, 0.07200150382011494, 0.10127522384232664, 0.07313600341056256, 0.07213997825029049, 0.07515875154987295, 0.07097493096234951, 0.07222987188303724, 0.07172657977393333, 0.07266695718181894, 0.06948143085266681, 0.07170205658420603, 0.07175027785149027, 0.07336655687144462, 0.07906123298279782, 0.07380894302053655, 0.0735062891815571, 0.09785956318708176, 0.07239167138617089, 0.0711729291905748, 0.0707796740722149, 0.07032899019565989, 0.07315828651189804, 0.06940919430331981, 0.07071957737207413, 0.07139077322914246, 0.07066063249998904, 0.07373150136876613, 0.06938056299026976, 0.07070591031236852, 0.07917940680016862, 0.07623969287948405, 0.07330419376809547, 0.07338580251374144, 0.07190100452367296, 0.07128205404002616, 0.06961505986908649, 0.06907738903735547, 0.0693120319158473, 0.07259493368737241, 0.07233929665798837, 0.06947769335609802, 0.06979025900363922, 0.06949847556175069, 0.07506592191280202, 0.06939243525266647, 0.0706119309080408, 0.06998341387890755, 0.06872656798743187, 0.07308717087862339, 0.07124672560615743, 0.0678854059982807, 0.06872642008548087, 0.06966567958923096, 0.06907767057418823, 0.06843232426871644, 0.06834749259213184, 0.07122366082795123, 0.06868539053074857, 0.07148468652938275, 0.06966887525421508, 0.06888434433556617, 0.06847443701104915, 0.06909052932516058, 0.06974470504420868, 0.0680277168433717, 0.06873828172683716, 0.06817573515024591, 0.06856508822517192, 0.06789459193006475, 0.07349156350531477, 0.0692654772007719, 0.0688784918252458, 0.06834465249421749, 0.07354782521724701, 0.06775475784819177, 0.07631958957682265, 0.07037852466740507, 0.07178093239347985, 0.06773605007440486, 0.06894920037147846, 0.06902266468139405, 0.06802196547071984, 0.06857317099545865, 0.06887875576602652, 0.06829253568294201, 0.06784361783177295, 0.06754463926908817, 0.0703954168773712, 0.06815776752030596, 0.06861519290411726, 0.068490225266903, 0.06846780345795002, 0.06785884341026874, 0.06869072578054794, 0.07056310741191214, 0.06897693967565577, 0.06851995982071186, 0.06832931935787201, 0.0682758149948526, 0.06834735372599135, 0.06768273562192917, 0.06759807578426727, 0.06894902092345218, 0.06850589050891552, 0.06919657201208967, 0.06965947484082365, 0.06995301566859509, 0.06806328115945166, 0.06860934610062457, 0.06856572485350548, 0.06959875768169443, 0.06924528691996919, 0.06768626981276146, 0.06822247391051435, 0.06890973385344161, 0.06875790972658928, 0.06775419207963537, 0.06783434558422008, 0.06966756728101284, 0.07026161348566096, 0.06910729360707263, 0.06918669214591067, 0.0690589447287803, 0.07012728395614218, 0.06810447224911223, 0.06800716100855077, 0.06827931169499742, 0.06792590244019285, 0.06869364434734304, 0.06970918051739956, 0.07102363398100467, 0.06781759049664153, 0.06788847595453262, 0.0682413379879708, 0.06771551801803265, 0.0696849063672918, 0.07028530997798797, 0.06857040904937907, 0.06921239910607642, 0.06790516715734563, 0.06790119758311738, 0.06794632321342509, 0.06817756799307276, 0.07093219871216631, 0.067724679537276, 0.06866619624990097, 0.06822185297595694, 0.06777820862988208, 0.06821134020673468, 0.06929065033476403, 0.06822629891177441, 0.06730986957220321, 0.06806804097079217, 0.0703528713672719, 0.06787506506798115, 0.06887736401342331, 0.06766554697396908, 0.06892348191839584, 0.06821880251803297, 0.06896905632729226, 0.0680127536996882, 0.06815579295792479, 0.0677112340927124, 0.06811175923398201, 0.07009662846301465, 0.06820347271067032, 0.06775877315630303, 0.06876449667392893, 0.06799067493448867, 0.06938573274206608, 0.0677181101859884, 0.07149604818922409, 0.06773120925781574, 0.06847504399558331, 0.068459837043539, 0.06903379600732884, 0.06793025056732462, 0.06832531792052249, 0.06916251001839942, 0.06828704317833514, 0.0680833877401149, 0.06883250319577278, 0.06805092920648291, 0.07289238052165255, 0.06889424679127146, 0.06901949580679549, 0.06903255778424283, 0.06784590904382949, 0.06810070827920386, 0.0694829044823951, 0.06844621294356407, 0.06801589245491839, 0.06820197514397033, 0.06758648466239585, 0.0690340428276265, 0.06943610342259103, 0.0676778163364593, 0.0677337054401002, 0.06819161582500377, 0.06910224084524398, 0.06958077427871684, 0.06834039757860468, 0.06874228998067532, 0.0679810901588582, 0.06803798865764699, 0.06806705273846363, 0.06808315582097844, 0.0683706612821589, 0.06914588809013367, 0.068375752010244, 0.06809630609573201, 0.06828132081539073, 0.06818620686201339, 0.06838721686855276, 0.06856218376692305, 0.07045309784564566, 0.06806669186087365, 0.06817959375838016, 0.0682412252781239, 0.06819866906772269, 0.06849995889562241, 0.06806617293586122, 0.06804205041299476, 0.06811062650794679, 0.06800452175926655, 0.06784090859458801, 0.06821730859736179, 0.06859666497466411, 0.07006086345682754, 0.06771380327483441, 0.07016069790784349, 0.06843731853556126, 0.06779256621573834, 0.0685927135513184, 0.06828779965005022, 0.06774950320733354, 0.06791697387048538, 0.06881409962760641, 0.06829457349599675, 0.06790688063235993, 0.06999821612175475, 0.06914047500554552, 0.06790358113481644, 0.06809375530227701, 0.06830014161607052, 0.06807563992890905, 0.06793110928636917, 0.06851005284710134, 0.0683088224143424, 0.06893241706680744, 0.06963190190652584, 0.06788492694180062, 0.06863043171928283, 0.06817871776032955, 0.06780693822718681, 0.06831534476356303, 0.06784677870096044, 0.06942871419039179, 0.06801735102496248, 0.06811365144366914, 0.06877398284825872, 0.06827007963302288, 0.06848719700219784, 0.06873707346459652, 0.06809258540260031, 0.06857937098817622, 0.06789941855884613, 0.06824171971133415, 0.06822096318640608, 0.06853925785485734, 0.06820749998726744, 0.0688747746196199, 0.06852498349357158, 0.06825340325210957, 0.0682645467050532, 0.06864633712362736, 0.06834622940167467, 0.0683953362576505, 0.06800866364798647, 0.06829123214838352, 0.06796894333464035, 0.06887771506258782, 0.06810788350536469, 0.06822363239653567, 0.06811717200152417, 0.06839767383768204, 0.06806554367884676, 0.06811397237346527, 0.06842932778787106, 0.06856601304830388, 0.06818327513781, 0.06868851866493834, 0.06809847334280927, 0.06793829338981751, 0.06831466010276308, 0.06838475817695577, 0.06790481491925869, 0.06850664927921396, 0.06932737694141712, 0.06813337574613855, 0.06920588214663749, 0.06903736578657273, 0.0682839813067558, 0.06811312522659911, 0.06844033380138113, 0.06829791563622495, 0.0683994397838065, 0.06812089856000657, 0.06836987175840012, 0.06812775467938566, 0.06815944572395467, 0.06829349601522405, 0.0683706299738681, 0.06818563047241658, 0.0681083528919423, 0.06943417142363305, 0.06818655275918067, 0.06808843344767043, 0.06817276712427749, 0.06857007757780399, 0.06895511518133447, 0.06864972380881613, 0.06808841323598902, 0.0683347171449915, 0.06899569865236892, 0.06945591007775449, 0.06829223662931869, 0.06839019408885469, 0.06837190164530531, 0.06801453232765198, 0.06790724301591833, 0.0685665932424525, 0.06909469530937519, 0.06816938448142498, 0.06884291340061958, 0.06851890699026432, 0.06848460515128806, 0.06830240746444845, 0.06837407610517868, 0.06848425132797119, 0.06809803193553965, 0.06833661680525922, 0.06814827730363988, 0.06834246075533806, 0.06874282245940351, 0.0680706466132022, 0.06800348398850319, 0.06839039985169755, 0.06842713422597722, 0.06869741101214226, 0.06998026386854496, 0.06899747680476372, 0.06817986736906335, 0.06845630404162914, 0.06818922109743382, 0.0687039556338432, 0.06809255052754219]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plotting training and validation losses\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "base_folder='/hpc/group/youlab/ks723/miniconda3/saved_models/logs'\n",
    "filename='losses_Pixel_32x32x3to32x32x4_dilRESNET_30k_newpatterns_seedtointermediate__Model_v1113_Cluster_GPU_tfData-1731542355.json'\n",
    "\n",
    "\n",
    "file=os.path.join(base_folder,filename)\n",
    "\n",
    "# Load the JSON file later if needed\n",
    "with open(file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# Extract train_losses and test_losses\n",
    "train_losses = data.get('train_losses', [])\n",
    "test_losses = data.get('test_losses', [])\n",
    "\n",
    "# Print the extracted losses\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Test Losses:\", test_losses)\n",
    "\n",
    "\n",
    "# Function to plot training and test losses with custom style\n",
    "def plot_losses(train_losses, test_losses):\n",
    "    sns.set(style='darkgrid', rc={\"axes.facecolor\": \"black\", \"grid.color\": \"grey\"})  # Setting the style\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    epochs = np.arange(1, len(train_losses) + 1)  # Assuming train_losses and test_losses are of the same length\n",
    "    \n",
    "    plt.plot(epochs, train_losses, label='Training Loss', color='cyan', linewidth=2)  # Thicker line for Training Loss\n",
    "    plt.plot(epochs, test_losses, label='Test Loss', color='magenta', linewidth=2)  # Thicker line for Test Loss\n",
    "    \n",
    "    plt.title('Training and Test Losses', color='black')\n",
    "    plt.xlabel('Epochs', color='black')\n",
    "    plt.ylabel('Loss', color='black')\n",
    "    plt.yscale('log')  # Set y-axis to log scale\n",
    "    \n",
    "    plt.tick_params(axis='x', colors='black')  # Change tick color to white\n",
    "    plt.tick_params(axis='y', colors='black')  # Change tick color to white\n",
    "    \n",
    "    plt.legend(facecolor='black', edgecolor='white', fontsize='medium', fancybox=True, framealpha=1, shadow=True, borderpad=1, labelcolor='white')\n",
    "    plt.show()\n",
    "\n",
    "# Make sure to call this function after the training loop\n",
    "plot_losses(train_losses, test_losses)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_pytorch_ipy_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
