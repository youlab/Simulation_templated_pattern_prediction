{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrediction file: Based on the trained neural network,see the performace on the testing set\\nMapping Seed-> Sim (Fig 2)\\n\\nGeneral workflow\\nLoad test data->Load trained dilResNet model and predict the output(are in latent representation) on test set-> \\nDecode into higher dimensional representation using SD VAE-> Display \\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prediction file: Based on the trained neural network,see the performace on the testing set\n",
    "Mapping Seed-> Sim (Fig 2)\n",
    "\n",
    "General workflow\n",
    "Load test data->Load trained dilResNet model and predict the output(are in latent representation) on test set-> \n",
    "Decode into higher dimensional representation using SD VAE-> Display \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-06 17:33:04,541] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# import libraries \n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import AutoencoderKL\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction from testing dataset\n",
    "\n",
    "'''\n",
    "Loading test data for model testing\n",
    "\n",
    "'''\n",
    "\n",
    "rfactor=256\n",
    "\n",
    "img_length=rfactor\n",
    "img_width=rfactor\n",
    "\n",
    "# for CentOS 8 cluster \n",
    "datadir_i2='/hpc/group/youlab/ks723/storage/MATLAB_SIMS/Sim_050924/Sim_input/intermediate/Tp3'\n",
    "datadir_o2=\"/hpc/group/youlab/ks723/storage/MATLAB_SIMS/Sim_050924/Sim_output\"\n",
    "\n",
    "path_i2=os.path.join(datadir_i2)\n",
    "path_o2=os.path.join(datadir_o2)\n",
    "\n",
    "ground_truth_data=[]\n",
    "input_data=[]\n",
    "\n",
    "# parameters for image\n",
    "\n",
    "img_shape_i=256  # to make image 256x256 after cropping the image\n",
    "img_shape_o=32   # to make image 32x32 after cropping the image \n",
    "\n",
    "\n",
    "# parameters for cropping output of sim hardcoded\n",
    "top_crop_i = 30\n",
    "bottom_crop_i = 30\n",
    "left_crop_i = 31\n",
    "right_crop_i = 30\n",
    "\n",
    "\n",
    "\n",
    "# do training with the next 10k images. load previous training point \n",
    "def create_ground_truth_data(top_crop, bottom_crop, left_crop, right_crop, start_index, end_index,path_i):\n",
    "    count = 0\n",
    "    img_filenames_i = sorted(os.listdir(path_i))\n",
    "\n",
    "    # Ensure end_index does not exceed the number of available images\n",
    "    end_index = min(end_index, len(img_filenames_i))\n",
    "\n",
    "    for img in img_filenames_i[start_index:end_index]:\n",
    "        img_array_i = cv2.imread(os.path.join(path_i, img), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        new_height = img_array_i.shape[0] - (top_crop + bottom_crop)\n",
    "        new_width = img_array_i.shape[1] - (left_crop + right_crop)\n",
    "\n",
    "        new_array_i = img_array_i[top_crop:top_crop + new_height, left_crop:left_crop + new_width]\n",
    "\n",
    "        new_array_i = cv2.resize(new_array_i, (img_length, img_width))\n",
    "\n",
    "        ground_truth_data.append([new_array_i])\n",
    "        count = count + 1\n",
    "        if count >= (end_index - start_index):\n",
    "            break\n",
    "\n",
    "\n",
    "## add some of the images from the main dataset that hasn't been used to train the network \n",
    "\n",
    "start_index = 70000  \n",
    "end_index = 70100 \n",
    "create_ground_truth_data(top_crop_i, bottom_crop_i, left_crop_i, right_crop_i, start_index, end_index,path_i2)\n",
    "\n",
    "top_crop_o=0\n",
    "bottom_crop_o=0\n",
    "left_crop_o=3\n",
    "right_crop_o=2\n",
    "\n",
    "def create_input_data(top_crop,bottom_crop,left_crop,right_crop, start_index, end_index,path_o):\n",
    "    \n",
    "    count=0\n",
    "\n",
    "    img_filenames_o = sorted(os.listdir(path_o))\n",
    "    # Ensure end_index does not exceed the number of available images\n",
    "    end_index = min(end_index, len(img_filenames_o))\n",
    "    \n",
    "    for img in img_filenames_o[start_index:end_index]:\n",
    "        img_array_o=cv2.imread(os.path.join(path_o,img),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        new_height = img_array_o.shape[0] - (top_crop + bottom_crop)\n",
    "        new_width = img_array_o.shape[1] - (left_crop + right_crop)\n",
    "        new_array_o = img_array_o[top_crop:top_crop+new_height, left_crop:left_crop+new_width]\n",
    "\n",
    "      \n",
    "        (T, new_array_o) = cv2.threshold(new_array_o, 0, 255,cv2.THRESH_BINARY| cv2.THRESH_OTSU)\n",
    "\n",
    "        input_data.append([new_array_o])\n",
    "        count=count+1\n",
    "        if count >= (end_index - start_index):\n",
    "            break\n",
    "\n",
    "\n",
    "start_index = 70000  \n",
    "end_index = 70100 \n",
    "create_input_data(top_crop_o, bottom_crop_o, left_crop_o, right_crop_o, start_index, end_index,path_o2)\n",
    "\n",
    "\n",
    "X=ground_truth_data  \n",
    "y=input_data\n",
    "\n",
    "X=(np.array(X).reshape(-1,1,img_shape_i,img_shape_i)) #/255.0  # last one is grayscale first minus one is all x\n",
    "y=(np.array(y).reshape(-1,1,img_shape_o,img_shape_o)) #/255.0\n",
    "\n",
    "# normalizing images here to be bw 0 and 1 \n",
    "\n",
    "X=X/255.0 \n",
    "y=y/255.0\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "\n",
    "y3=y.repeat(1, 3, 1, 1)\n",
    "y4=y.repeat(1, 4, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining dilResNet model for loading model weights\n",
    "'''\n",
    "# Dilated Basic Block similar to PDEArena\n",
    "class PDEArenaDilatedBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, dilation_rates, activation=nn.ReLU, norm=True):\n",
    "        super(PDEArenaDilatedBlock, self).__init__()\n",
    "\n",
    "        # Create dilated convolution layers with specified dilation rates\n",
    "        self.dilated_layers = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_planes if i == 0 else out_planes, \n",
    "                out_planes, \n",
    "                kernel_size=3, \n",
    "                padding=rate, \n",
    "                dilation=rate, \n",
    "                bias=False\n",
    "            )\n",
    "            for i, rate in enumerate(dilation_rates)\n",
    "        ])\n",
    "        \n",
    "        # Normalization and Activation layers\n",
    "        self.norm_layers = nn.ModuleList([nn.BatchNorm2d(out_planes) if norm else nn.Identity() for _ in dilation_rates])\n",
    "        self.activation = activation(inplace=True)\n",
    "\n",
    "        # Shortcut (1x1 convolution if input and output planes differ)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_planes) if norm else nn.Identity()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer, norm in zip(self.dilated_layers, self.norm_layers):\n",
    "            out = self.activation(norm(layer(out)))\n",
    "        return out + self.shortcut(x)  # Residual connection\n",
    "\n",
    "# Dilated ResNet with Adjustable Layers and Blocks\n",
    "class PDEArenaDilatedResNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=64, num_blocks=15, dilation_rates=[1, 2, 4, 8], activation=nn.ReLU, norm=True):\n",
    "        super(PDEArenaDilatedResNet, self).__init__()\n",
    "        \n",
    "        self.in_conv = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)  # Input layer\n",
    "        \n",
    "        # Stack of dilated blocks\n",
    "        self.layers = nn.Sequential(\n",
    "            *[PDEArenaDilatedBlock(hidden_channels, hidden_channels, dilation_rates, activation=activation, norm=norm) for _ in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(hidden_channels, out_channels, kernel_size=3, padding=1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_conv(x)\n",
    "        x = self.layers(x)\n",
    "        return self.out_conv(x)\n",
    "\n",
    "\n",
    "model = PDEArenaDilatedResNet(\n",
    "    in_channels=3,               # Input channels\n",
    "    out_channels=4,              # Output channels \n",
    "    hidden_channels=64,          # Number of hidden channels\n",
    "    num_blocks=15,               # Number of dilated blocks \n",
    "    dilation_rates=[1, 2, 4, 8], # Dilation rates for multi-scale feature capture\n",
    "    activation=nn.ReLU,          # Activation function\n",
    "    norm=True                    # Use BatchNorm after each convolution\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 7.8950 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define pre-trained SD VAE for decoding orginal predicted patterns from the latent predicted patterns\n",
    "dilResnet will predict latent representation of patterns \n",
    "\n",
    "'''\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "\n",
    "# first load the VAE\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "vae.to(device)\n",
    "\n",
    "def encode_img(input_img):\n",
    "    input_img = input_img.repeat(3, 1, 1)\n",
    "    \n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    if len(input_img.shape)<4:\n",
    "        input_img = input_img.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(input_img*2 - 1) # Note scaling  # to make outputs from -1 to 1 \n",
    "    return 0.18215 * latent.latent_dist.sample()    \n",
    "\n",
    "\n",
    "\n",
    "def decode_img(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)    # to make outputs from 0 to 1 \n",
    "    image = image.detach()\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "NAME='Pixel_32x32x3to32x32x4_dilRESNET_30k_newpatterns_seedtointermediate__Model_v1113_Cluster_GPU_tfData-1731542355'\n",
    "model.load_state_dict(torch.load(f'/hpc/group/youlab/ks723/miniconda3/saved_models/trained/{NAME}.pt'))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "start_time=time.perf_counter()\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(y3), batch_size):\n",
    "        batch = y3[i:i+batch_size].to(device) \n",
    "        predicted_latents=model(batch)  #y3 is the input seed with 3 channels for the UNET \n",
    "\n",
    "end_time=time.perf_counter()\n",
    "\n",
    "print(f\"Prediction time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# predicted_latents_rescaled=predicted_latents\n",
    "\n",
    "# # use the vae decoder to convert the encoded images to final patterns\n",
    "\n",
    "# pred_images=decode_img(predicted_latents_rescaled)\n",
    "\n",
    "\n",
    "# '''\n",
    "# Functions for displaying seed, actual simulation and predicted simulation\n",
    "# '''\n",
    "# # for generation see first row seed, second row final patterns(ground truth), third row generated patterns( predicted output)\n",
    "\n",
    "\n",
    "# def tensor_to_pil_v2(tensor):\n",
    "#     tensor = tensor.permute(1, 2, 0)  # Convert to (height, width, channels)\n",
    "#     img = (tensor.cpu().numpy() * 255).astype('uint8')\n",
    "#     return Image.fromarray(img.squeeze())\n",
    "\n",
    "# def display_predicted_images(input_seed,final_patterns,pred_images, num_samples):\n",
    "   \n",
    "#     plt.figure(figsize=(6*num_samples/3,6))\n",
    "#     plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "\n",
    "#     for i in range(num_samples):\n",
    "\n",
    "#         image_i=tensor_to_pil_v2(input_seed[i,:,:,:])\n",
    "#         image_o=tensor_to_pil_v2(final_patterns[i,:,:,:])\n",
    "#         image_p= tensor_to_pil_v2(pred_images[i,:,:,:].to(\"cpu\"))\n",
    "        \n",
    "#         ax = plt.subplot(3, num_samples, i + 1)\n",
    "\n",
    "#         plt.imshow(image_i)\n",
    "#         plt.gray()\n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "\n",
    "#         ax = plt.subplot(3, num_samples, i + 1+num_samples)\n",
    "\n",
    "#         plt.imshow(image_p,cmap=\"gray\")\n",
    "        \n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "#         print(np.array(image_p).shape)\n",
    "    \n",
    "\n",
    "#         ax=plt.subplot(3, num_samples, i + 1+ num_samples+ num_samples)\n",
    "\n",
    "        \n",
    "#         plt.imshow(image_o)\n",
    "#         plt.gray()\n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# selected_indices=[4,5,2,10,14] \n",
    "# # Display samples from train dataset\n",
    "# display_predicted_images(y3[selected_indices,:,:,:],X[selected_indices,:,:,:], pred_images[selected_indices,:,:,:] ,len(selected_indices))   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 706.89x520.157 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the benchmarking graph \n",
    "\n",
    "# for 100 images, the time for prediction using GPU is 0.15s, and using CPU is 8.0s->100 times slower\n",
    "\n",
    "# Prediction with trained ML model, using CPU + generation of MATLAB scripts \n",
    "\n",
    "# Parallel total time: 29 min for 100 images. (times from email slurm jobs) \n",
    "# Serial total time: 04 hours, 22 min for 100 images. (times from email slurm jobs)\n",
    "\n",
    "#for A4 size\n",
    "height_figure= 4.404 /2.54 # in inches\n",
    "width_figure= 5.985 /2.54 # in inches\n",
    "fig,ax=plt.subplots(1,1,figsize=(width_figure,height_figure),dpi=300) # A4 size in inches\n",
    "\n",
    "times=[8/100,29*60/100,(4*60+22)*60/100] # in seconds\n",
    "labels=['ML surrogate','PDE Simulation(Parallel computing)','PDE simulation']\n",
    "\n",
    "\n",
    "bar_colors=['#a6e7f2fa','#707070b5','#707070b5']\n",
    "ax.bar(labels,times,color=bar_colors,edgecolor='black',linewidth=0.3)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='y', labelsize=9)\n",
    "# hide the x ticks\n",
    "ax.set_xticks([])\n",
    "# remove the top border\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# ax.tick_params(axis='x', labelsize=15)\n",
    "# ax.set_ylabel('Time (seconds)',fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_PA_patternprediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
