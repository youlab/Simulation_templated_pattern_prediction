{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   \n",
    "\n",
    "import torch \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark   = False\n",
    "torch.use_deterministic_algorithms(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-11 21:13:43,031] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "logging improved.\n",
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 729397049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:13<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> intermediates type: <class 'dict'>\n",
      ">>> keys: ['x_inter', 'pred_x0']\n",
      "  'x_inter': <class 'list'> with length 51; first element type <class 'torch.Tensor'>\n",
      "  'pred_x0': <class 'list'> with length 51; first element type <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "Global seed set to 729397049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Data shape for DDIM sampling is (2, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:11<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> intermediates type: <class 'dict'>\n",
      ">>> keys: ['x_inter', 'pred_x0']\n",
      "  'x_inter': <class 'list'> with length 51; first element type <class 'torch.Tensor'>\n",
      "  'pred_x0': <class 'list'> with length 51; first element type <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n",
      "/hpc/dctrl/ks723/miniconda3/envs/pytorch_PA_patternprediction/lib/python3.10/site-packages/gradio/components.py:1878: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from share import *\n",
    "import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "from omegaconf import OmegaConf\n",
    "from cldm.cldm import ControlLDM  # Your model class\n",
    "import tempfile, imageio\n",
    "\n",
    "from cldm.config import CKPT_PATH\n",
    "\n",
    "# modified code\n",
    "\n",
    "yaml_config = \"./models/cldm_v15.yaml\"           # YAML configuration file\n",
    "ckpt_path= CKPT_PATH\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load the Model from Checkpoint\n",
    "# ------------------------------\n",
    "config_yaml = OmegaConf.load(yaml_config)\n",
    "params = OmegaConf.to_container(config_yaml.model.params, resolve=True)\n",
    "model = ControlLDM.load_from_checkpoint(ckpt_path, **params)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# torch.manual_seed(seed_value)\n",
    "# torch.cuda.manual_seed_all(seed_value)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark   = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Set up the DDIM sampler.\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "def latent_to_rgba(z_bchw: torch.Tensor) -> np.ndarray:\n",
    "    # z_bchw: [1,4,h,w]\n",
    "    z = z_bchw[0].float()                # [4,h,w]\n",
    "    c, h, w = z.shape\n",
    "    assert c == 4, \"expected 4-channel latents\"\n",
    "    chans = []\n",
    "    for k in range(4):\n",
    "        m = z[k]\n",
    "        m = (m - m.min()) / (m.max() - m.min() + 1e-8)   # 0..1 per-channel\n",
    "        chans.append(m)\n",
    "    rgba = torch.stack(chans, 0)                          # [4,h,w]\n",
    "    rgba = (rgba.permute(1,2,0) * 255).byte().cpu().numpy()  # (h,w,4) uint8\n",
    "    return rgba\n",
    "\n",
    "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n",
    "    with torch.no_grad():\n",
    "        img = resize_image(HWC3(input_image), image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        print(type(img))\n",
    "        # detected_map = np.zeros_like(img, dtype=np.uint8)\n",
    "        # detected_map[np.min(img, axis=2) < 127] = 255\n",
    "\n",
    "        control = torch.from_numpy(img.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
    "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
    "                                                     shape, cond, verbose=False, eta=eta,\n",
    "                                                     log_every_t=1,\n",
    "                                                     unconditional_guidance_scale=scale,\n",
    "                                                     unconditional_conditioning=un_cond)\n",
    "        \n",
    "        print(\">>> intermediates type:\", type(intermediates))\n",
    "        if isinstance(intermediates, dict):\n",
    "            print(\">>> keys:\", list(intermediates.keys()))\n",
    "        for k,v in intermediates.items():\n",
    "            print(f\"  {k!r}: {type(v)} with length {len(v)}; first element type {type(v[0])}\")\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "\n",
    "\n",
    "        # decode each x_inter latent \n",
    "        latent_list = intermediates.get('x_inter', [])\n",
    "\n",
    "        # two tracks: [sample0_frames, sample1_frames]\n",
    "        frames_dec = [[], []]\n",
    "        frames_lat = [[], []]\n",
    "\n",
    "        for z in latent_list:\n",
    "            # sample 0\n",
    "            if z.shape[0] >= 1:\n",
    "                z0 = z[0:1].to(model.device)                       # [1,4,h,w]\n",
    "                frames_lat[0].append(latent_to_rgba(z0))\n",
    "                img0_t = model.decode_first_stage(z0)[0]            # CHW, [-1,1]\n",
    "                img0 = ((img0_t.clamp(-1,1)+1)/2).permute(1,2,0).mul(255).byte().cpu().numpy()\n",
    "                frames_dec[0].append(img0)\n",
    "\n",
    "            # sample 1\n",
    "            if z.shape[0] >= 2:\n",
    "                z1 = z[1:2].to(model.device)\n",
    "                frames_lat[1].append(latent_to_rgba(z1))\n",
    "                img1_t = model.decode_first_stage(z1)[0]\n",
    "                img1 = ((img1_t.clamp(-1,1)+1)/2).permute(1,2,0).mul(255).byte().cpu().numpy()\n",
    "                frames_dec[1].append(img1)\n",
    "\n",
    "        def write_gif(frames):\n",
    "            tmp = tempfile.NamedTemporaryFile(suffix=\".gif\", delete=False); tmp.close()\n",
    "            imageio.mimsave(tmp.name, frames, format=\"GIF-PIL\", duration=1/12, loop=0)\n",
    "            return tmp.name\n",
    "\n",
    "        s0_dec = write_gif(frames_dec[0]); s0_lat = write_gif(frames_lat[0])\n",
    "        if len(frames_dec[1]) and len(frames_lat[1]):\n",
    "            s1_dec = write_gif(frames_dec[1]); s1_lat = write_gif(frames_lat[1])\n",
    "        else:\n",
    "            s1_dec, s1_lat = s0_dec, s0_lat\n",
    "        downloads = [s0_dec, s0_lat, s1_dec, s1_lat]\n",
    "\n",
    "    return results, s0_dec, s0_lat, s1_dec, s1_lat, downloads\n",
    "\n",
    "    # return results ,tmp_dec.name, tmp_lat.name\n",
    "\n",
    "\n",
    "block = gr.Blocks().queue()\n",
    "\n",
    "with block:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"## Control Stable Diffusion between two Simulated Parameters\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_image = gr.Image(source='upload', type=\"numpy\")\n",
    "            prompt = gr.Textbox(label=\"Prompt\")\n",
    "            run_button = gr.Button(label=\"Run\")\n",
    "            with gr.Accordion(\"Advanced options\", open=False):\n",
    "                num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
    "                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n",
    "                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n",
    "                guess_mode = gr.Checkbox(label='Guess Mode', value=False) \n",
    "                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
    "                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n",
    "                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n",
    "                eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n",
    "                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality, extremely detailed')\n",
    "                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
    "                                      value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
    "        with gr.Column():\n",
    "            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n",
    "            video_s0_dec = gr.Video(label=\"Sample 1 • Decoded ▶️\", format=\"gif\")\n",
    "            video_s0_lat = gr.Video(label=\"Sample 1 • Latent ▶️\",  format=\"gif\")\n",
    "            video_s1_dec = gr.Video(label=\"Sample 2 • Decoded ▶️\", format=\"gif\")\n",
    "            video_s1_lat = gr.Video(label=\"Sample 2 • Latent ▶️\",  format=\"gif\")\n",
    "            downloads     = gr.Files(label=\"Download videos\")   \n",
    "    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n",
    "    run_button.click(fn=process, inputs=ips, outputs=[result_gallery, video_s0_dec, video_s0_lat, video_s1_dec, video_s1_lat, downloads])\n",
    "\n",
    "\n",
    "block.queue().launch(debug='True',server_name='0.0.0.0',share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_PA_patternprediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
