{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-06 23:41:15,411] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "logging improved.\n",
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/gradio/routes.py\", line 337, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/gradio/blocks.py\", line 1015, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/gradio/blocks.py\", line 833, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_1079534/521077035.py\", line 71, in process\n",
      "    emulated_pattern = predict_from_seed(seed_img)\n",
      "  File \"/hpc/dctrl/ks723/Huggingface_repos/ControlNet_repo/controlnet_repo/prediction_seedtosim.py\", line 30, in prediction\n",
      "    predicted_images = predict_from_seed(seed_image, model, vae, device)\n",
      "  File \"/hpc/dctrl/ks723/Huggingface_repos/ControlNet_repo/controlnet_repo/predict_util.py\", line 18, in predict_from_seed\n",
      "    predicted_images = decode_img(predicted_latents, vae)\n",
      "  File \"/hpc/dctrl/ks723/Huggingface_repos/ControlNet_repo/controlnet_repo/vae_util.py\", line 17, in decode_img\n",
      "    image = vae.decode(latents).sample\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\", line 270, in decode\n",
      "    decoded = self._decode(z).sample\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\", line 257, in _decode\n",
      "    dec = self.decoder(z)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/models/vae.py\", line 266, in forward\n",
      "    sample = self.mid_block(sample, latent_embeds)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py\", line 538, in forward\n",
      "    hidden_states = attn(hidden_states, temb=temb)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/models/attention_processor.py\", line 322, in forward\n",
      "    return self.processor(\n",
      "  File \"/hpc/dctrl/ks723/miniconda3/envs/test_pytorch_ipy_v2/lib/python3.10/site-packages/diffusers/models/attention_processor.py\", line 1117, in __call__\n",
      "    hidden_states = F.scaled_dot_product_attention(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 23.58 GiB total capacity; 22.56 GiB already allocated; 639.38 MiB free; 22.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from share import *\n",
    "import config\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "\n",
    "\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "from omegaconf import OmegaConf\n",
    "from cldm.cldm import ControlLDM  # Your model class\n",
    "\n",
    "# Import custom functions:l=\n",
    "from image_to_seed import image_to_seed\n",
    "from prediction_seedtosim import prediction as predict_from_seed\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Load the ControlNet model from checkpoint\n",
    "# ------------------------------\n",
    "yaml_config = \"./models/cldm_v15.yaml\"           # YAML configuration file\n",
    "# ckpt_path = \"/hpc/dctrl/ks723/Huggingface_repos/ControlNet_repo/controlnet_repo/lightning_logs/version_25478352/checkpoints/epoch=4-step=94129.ckpt\"  # sim1tosim2\n",
    "ckpt_path = '/hpc/dctrl/ks723/Huggingface_repos/ControlNet_repo/controlnet_repo/lightning_logs/version_25478850/checkpoints/epoch=4-step=51124.ckpt'\n",
    "\n",
    "config_yaml = OmegaConf.load(yaml_config)\n",
    "params = OmegaConf.to_container(config_yaml.model.params, resolve=True)\n",
    "model = ControlLDM.load_from_checkpoint(ckpt_path, **params)\n",
    "model = model.cuda()\n",
    "\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n",
    "    with torch.no_grad():\n",
    "  \n",
    "        print((input_image.shape))\n",
    "\n",
    "        img = resize_image(HWC3(input_image[:, :, 0]), 256)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "        inverted_img= 255-img\n",
    "        img=inverted_img\n",
    "\n",
    "\n",
    "        # --- Step 2: Image-to-Seed conversion ---\n",
    "        # Convert the processed canvas into a seed image.\n",
    "        \n",
    "        seed_img = image_to_seed(img, num_dots=50, min_area=40)\n",
    "        \n",
    "        # --- Step 3: ResNet Emulation ---\n",
    "    \n",
    "        emulated_pattern = predict_from_seed(seed_img)\n",
    "        predicted_img = emulated_pattern[0].permute(1, 2, 0).cpu().numpy()  # Convert to HxWxC numpy image\n",
    "\n",
    "        img=predicted_img\n",
    "\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        \n",
    "        # Use the processed image directly (without inverting the mask)\n",
    "        control = torch.from_numpy(img.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\n",
    "            \"c_concat\": [control],\n",
    "            \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]\n",
    "        }\n",
    "        un_cond = {\n",
    "            \"c_concat\": None if guess_mode else [control],\n",
    "            \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]\n",
    "        }\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = ([strength * (0.825 ** float(12 - i)) for i in range(13)]\n",
    "                                  if guess_mode else ([strength] * 13))\n",
    "        samples, intermediates = ddim_sampler.sample(\n",
    "            ddim_steps, num_samples, shape, cond, verbose=False, eta=eta,\n",
    "            unconditional_guidance_scale=scale, unconditional_conditioning=un_cond\n",
    "        )\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy()\n",
    "        x_samples = x_samples.clip(0, 255).astype(np.uint8)\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_canvas(w, h):\n",
    "    return np.zeros(shape=(h, w, 3), dtype=np.uint8) + 255\n",
    "\n",
    "# ------------------------------\n",
    "# Gradio Interface Setup\n",
    "# ------------------------------\n",
    "block = gr.Blocks().queue()\n",
    "\n",
    "with block:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"## Control Stable Diffusion (Drawing Input)\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # Use 'sketch' tool to allow drawing.\n",
    "\n",
    "            canvas_width = gr.Slider(label=\"Canvas Width\", minimum=256, maximum=1024, value=512, step=1)\n",
    "            canvas_height = gr.Slider(label=\"Canvas Height\", minimum=256, maximum=1024, value=512, step=1)\n",
    "            create_button = gr.Button(label=\"Start\", value='Open drawing canvas!')\n",
    "\n",
    "\n",
    "            input_image = gr.Image(source='canvas', tool=\"sketch\", type=\"numpy\", shape=(256,256))\n",
    "          \n",
    "            create_button.click(fn=create_canvas, outputs=[input_image], inputs=[canvas_width, canvas_height])  #inputs=[canvas_width, canvas_height]\n",
    "\n",
    "            prompt = gr.Textbox(label=\"Prompt\")\n",
    "            run_button = gr.Button(label=\"Run\")\n",
    "\n",
    "            \n",
    "          \n",
    "        \n",
    "\n",
    "            with gr.Accordion(\"Advanced options\", open=False):\n",
    "                num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
    "                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n",
    "                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n",
    "                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n",
    "                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
    "                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n",
    "                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n",
    "                eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n",
    "                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality, extremely detailed')\n",
    "                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
    "                                      value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
    "        with gr.Column():\n",
    "            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n",
    "    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n",
    "    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n",
    "\n",
    "block.queue().launch(debug=True, server_name='0.0.0.0', share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_pytorch_ipy_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
